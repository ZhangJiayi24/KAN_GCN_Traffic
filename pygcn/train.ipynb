{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05cd84d-4aab-40d7-85c1-08864d0baebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import load_data, accuracy\n",
    "from models import GCNWithKAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a4db0f-9688-4dea-af66-822358fb0b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/pygcn/pygcn/utils.py:80: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1724789220573/work/torch/csrc/utils/tensor_new.cpp:641.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9815 acc_train: 0.1500 loss_val: 1.9307 acc_val: 0.1567 time: 0.3779s\n",
      "Epoch: 0002 loss_train: 1.9196 acc_train: 0.1929 loss_val: 1.8803 acc_val: 0.1567 time: 0.0121s\n",
      "Epoch: 0003 loss_train: 1.8776 acc_train: 0.1857 loss_val: 1.8471 acc_val: 0.1567 time: 0.0117s\n",
      "Epoch: 0004 loss_train: 1.8566 acc_train: 0.1929 loss_val: 1.8260 acc_val: 0.3500 time: 0.0116s\n",
      "Epoch: 0005 loss_train: 1.8605 acc_train: 0.2643 loss_val: 1.8168 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0006 loss_train: 1.8490 acc_train: 0.2857 loss_val: 1.8235 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0007 loss_train: 1.8402 acc_train: 0.2786 loss_val: 1.8308 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0008 loss_train: 1.8394 acc_train: 0.2857 loss_val: 1.8143 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0009 loss_train: 1.8184 acc_train: 0.2786 loss_val: 1.8032 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0010 loss_train: 1.8473 acc_train: 0.2786 loss_val: 1.8024 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0011 loss_train: 1.8036 acc_train: 0.2929 loss_val: 1.8006 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0012 loss_train: 1.8204 acc_train: 0.2929 loss_val: 1.8002 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0013 loss_train: 1.8179 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0014 loss_train: 1.8260 acc_train: 0.2929 loss_val: 1.8060 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0015 loss_train: 1.8031 acc_train: 0.2929 loss_val: 1.8038 acc_val: 0.3500 time: 0.0128s\n",
      "Epoch: 0016 loss_train: 1.8147 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0116s\n",
      "Epoch: 0017 loss_train: 1.8123 acc_train: 0.2929 loss_val: 1.7986 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0018 loss_train: 1.8173 acc_train: 0.2929 loss_val: 1.7970 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0019 loss_train: 1.7966 acc_train: 0.2929 loss_val: 1.7953 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0020 loss_train: 1.8260 acc_train: 0.2929 loss_val: 1.7981 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0021 loss_train: 1.8027 acc_train: 0.2929 loss_val: 1.7989 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0022 loss_train: 1.7990 acc_train: 0.2929 loss_val: 1.7983 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0023 loss_train: 1.8192 acc_train: 0.2929 loss_val: 1.7990 acc_val: 0.3500 time: 0.0112s\n",
      "Epoch: 0024 loss_train: 1.8050 acc_train: 0.2929 loss_val: 1.7990 acc_val: 0.3500 time: 0.0112s\n",
      "Epoch: 0025 loss_train: 1.8034 acc_train: 0.2929 loss_val: 1.7994 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0026 loss_train: 1.8078 acc_train: 0.2929 loss_val: 1.8003 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0027 loss_train: 1.8074 acc_train: 0.2929 loss_val: 1.8011 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0028 loss_train: 1.8047 acc_train: 0.2929 loss_val: 1.7996 acc_val: 0.3500 time: 0.0112s\n",
      "Epoch: 0029 loss_train: 1.8003 acc_train: 0.2929 loss_val: 1.7966 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0030 loss_train: 1.8020 acc_train: 0.2929 loss_val: 1.7935 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0031 loss_train: 1.8036 acc_train: 0.2929 loss_val: 1.7912 acc_val: 0.3500 time: 0.0130s\n",
      "Epoch: 0032 loss_train: 1.8127 acc_train: 0.2929 loss_val: 1.7915 acc_val: 0.3500 time: 0.0114s\n",
      "Epoch: 0033 loss_train: 1.7917 acc_train: 0.2929 loss_val: 1.7921 acc_val: 0.3500 time: 0.0116s\n",
      "Epoch: 0034 loss_train: 1.7851 acc_train: 0.2929 loss_val: 1.7916 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0035 loss_train: 1.7790 acc_train: 0.2929 loss_val: 1.7915 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0036 loss_train: 1.8042 acc_train: 0.2929 loss_val: 1.7898 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0037 loss_train: 1.8027 acc_train: 0.2929 loss_val: 1.7876 acc_val: 0.3500 time: 0.0113s\n",
      "Epoch: 0038 loss_train: 1.7943 acc_train: 0.2929 loss_val: 1.7881 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0039 loss_train: 1.7917 acc_train: 0.3000 loss_val: 1.7881 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0040 loss_train: 1.7901 acc_train: 0.2857 loss_val: 1.7872 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0041 loss_train: 1.7734 acc_train: 0.3000 loss_val: 1.7862 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0042 loss_train: 1.7724 acc_train: 0.2857 loss_val: 1.7853 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0043 loss_train: 1.7962 acc_train: 0.2929 loss_val: 1.7845 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0044 loss_train: 1.7861 acc_train: 0.2929 loss_val: 1.7855 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0045 loss_train: 1.7824 acc_train: 0.2929 loss_val: 1.7903 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0046 loss_train: 1.7840 acc_train: 0.2643 loss_val: 1.7876 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0047 loss_train: 1.8130 acc_train: 0.2786 loss_val: 1.7904 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0048 loss_train: 1.7896 acc_train: 0.2929 loss_val: 1.7936 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0049 loss_train: 1.7721 acc_train: 0.3000 loss_val: 1.7925 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0050 loss_train: 1.7942 acc_train: 0.3000 loss_val: 1.7877 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0051 loss_train: 1.7720 acc_train: 0.3071 loss_val: 1.7819 acc_val: 0.3500 time: 0.0121s\n",
      "Epoch: 0052 loss_train: 1.7674 acc_train: 0.3143 loss_val: 1.7769 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0053 loss_train: 1.7661 acc_train: 0.2929 loss_val: 1.7748 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0054 loss_train: 1.7595 acc_train: 0.3000 loss_val: 1.7759 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0055 loss_train: 1.7772 acc_train: 0.2929 loss_val: 1.7761 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0056 loss_train: 1.7464 acc_train: 0.2929 loss_val: 1.7814 acc_val: 0.3667 time: 0.0111s\n",
      "Epoch: 0057 loss_train: 1.7701 acc_train: 0.3143 loss_val: 1.7755 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0058 loss_train: 1.7432 acc_train: 0.3214 loss_val: 1.7755 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0059 loss_train: 1.7280 acc_train: 0.3071 loss_val: 1.7738 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0060 loss_train: 1.7444 acc_train: 0.2929 loss_val: 1.7721 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0061 loss_train: 1.7433 acc_train: 0.2929 loss_val: 1.7660 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0062 loss_train: 1.7296 acc_train: 0.3286 loss_val: 1.7643 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0063 loss_train: 1.7278 acc_train: 0.3500 loss_val: 1.7671 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0064 loss_train: 1.7301 acc_train: 0.3286 loss_val: 1.7706 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0065 loss_train: 1.7068 acc_train: 0.3286 loss_val: 1.7764 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0066 loss_train: 1.7179 acc_train: 0.3143 loss_val: 1.7704 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0067 loss_train: 1.6791 acc_train: 0.3571 loss_val: 1.7615 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0068 loss_train: 1.6859 acc_train: 0.3857 loss_val: 1.7564 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0069 loss_train: 1.6557 acc_train: 0.3929 loss_val: 1.7537 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0070 loss_train: 1.6914 acc_train: 0.4143 loss_val: 1.7466 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0071 loss_train: 1.6716 acc_train: 0.4000 loss_val: 1.7445 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0072 loss_train: 1.6396 acc_train: 0.3786 loss_val: 1.7599 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0073 loss_train: 1.6525 acc_train: 0.3571 loss_val: 1.7815 acc_val: 0.3533 time: 0.0113s\n",
      "Epoch: 0074 loss_train: 1.6441 acc_train: 0.4000 loss_val: 1.7866 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0075 loss_train: 1.6424 acc_train: 0.3786 loss_val: 1.7698 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0076 loss_train: 1.6220 acc_train: 0.4786 loss_val: 1.7371 acc_val: 0.3867 time: 0.0111s\n",
      "Epoch: 0077 loss_train: 1.6287 acc_train: 0.3786 loss_val: 1.7603 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0078 loss_train: 1.6042 acc_train: 0.3929 loss_val: 1.7645 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0079 loss_train: 1.5885 acc_train: 0.3571 loss_val: 1.7518 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0080 loss_train: 1.5266 acc_train: 0.4357 loss_val: 1.7481 acc_val: 0.3700 time: 0.0111s\n",
      "Epoch: 0081 loss_train: 1.5455 acc_train: 0.4571 loss_val: 1.8039 acc_val: 0.4500 time: 0.0111s\n",
      "Epoch: 0082 loss_train: 1.5124 acc_train: 0.4786 loss_val: 1.7894 acc_val: 0.4433 time: 0.0111s\n",
      "Epoch: 0083 loss_train: 1.5218 acc_train: 0.4357 loss_val: 1.7536 acc_val: 0.3700 time: 0.0111s\n",
      "Epoch: 0084 loss_train: 1.5252 acc_train: 0.4929 loss_val: 1.7238 acc_val: 0.3667 time: 0.0111s\n",
      "Epoch: 0085 loss_train: 1.4757 acc_train: 0.4714 loss_val: 1.7170 acc_val: 0.3600 time: 0.0111s\n",
      "Epoch: 0086 loss_train: 1.4890 acc_train: 0.4571 loss_val: 1.7962 acc_val: 0.4033 time: 0.0111s\n",
      "Epoch: 0087 loss_train: 1.4951 acc_train: 0.4929 loss_val: 1.8172 acc_val: 0.2133 time: 0.0111s\n",
      "Epoch: 0088 loss_train: 1.4682 acc_train: 0.5357 loss_val: 1.7804 acc_val: 0.4333 time: 0.0111s\n",
      "Epoch: 0089 loss_train: 1.4736 acc_train: 0.4643 loss_val: 1.7984 acc_val: 0.4333 time: 0.0111s\n",
      "Epoch: 0090 loss_train: 1.4224 acc_train: 0.5143 loss_val: 1.8169 acc_val: 0.2033 time: 0.0111s\n",
      "Epoch: 0091 loss_train: 1.4144 acc_train: 0.5857 loss_val: 1.7435 acc_val: 0.4867 time: 0.0114s\n",
      "Epoch: 0092 loss_train: 1.4136 acc_train: 0.4714 loss_val: 1.7569 acc_val: 0.4600 time: 0.0111s\n",
      "Epoch: 0093 loss_train: 1.3611 acc_train: 0.5643 loss_val: 1.7955 acc_val: 0.4100 time: 0.0111s\n",
      "Epoch: 0094 loss_train: 1.4034 acc_train: 0.5214 loss_val: 1.7535 acc_val: 0.3933 time: 0.0111s\n",
      "Epoch: 0095 loss_train: 1.3852 acc_train: 0.4571 loss_val: 1.7916 acc_val: 0.2567 time: 0.0111s\n",
      "Epoch: 0096 loss_train: 1.3722 acc_train: 0.5571 loss_val: 1.8877 acc_val: 0.1267 time: 0.0111s\n",
      "Epoch: 0097 loss_train: 1.3292 acc_train: 0.6143 loss_val: 1.8918 acc_val: 0.1300 time: 0.0111s\n",
      "Epoch: 0098 loss_train: 1.2837 acc_train: 0.5143 loss_val: 1.8351 acc_val: 0.2067 time: 0.0111s\n",
      "Epoch: 0099 loss_train: 1.2620 acc_train: 0.4786 loss_val: 1.8509 acc_val: 0.1867 time: 0.0112s\n",
      "Epoch: 0100 loss_train: 1.2729 acc_train: 0.6500 loss_val: 1.8235 acc_val: 0.2233 time: 0.0111s\n",
      "Epoch: 0101 loss_train: 1.2868 acc_train: 0.5714 loss_val: 1.7929 acc_val: 0.2167 time: 0.0111s\n",
      "Epoch: 0102 loss_train: 1.2430 acc_train: 0.5071 loss_val: 1.8917 acc_val: 0.1300 time: 0.0111s\n",
      "Epoch: 0103 loss_train: 1.2318 acc_train: 0.6214 loss_val: 1.9105 acc_val: 0.1733 time: 0.0111s\n",
      "Epoch: 0104 loss_train: 1.2177 acc_train: 0.6500 loss_val: 1.9902 acc_val: 0.1800 time: 0.0111s\n",
      "Epoch: 0105 loss_train: 1.1947 acc_train: 0.5786 loss_val: 2.0751 acc_val: 0.1333 time: 0.0111s\n",
      "Epoch: 0106 loss_train: 1.1670 acc_train: 0.6429 loss_val: 2.0297 acc_val: 0.1533 time: 0.0111s\n",
      "Epoch: 0107 loss_train: 1.1615 acc_train: 0.6429 loss_val: 2.0014 acc_val: 0.2433 time: 0.0111s\n",
      "Epoch: 0108 loss_train: 1.1058 acc_train: 0.6500 loss_val: 1.9055 acc_val: 0.2267 time: 0.0111s\n",
      "Epoch: 0109 loss_train: 1.1541 acc_train: 0.6571 loss_val: 2.0188 acc_val: 0.1900 time: 0.0113s\n",
      "Epoch: 0110 loss_train: 1.1565 acc_train: 0.6500 loss_val: 2.0072 acc_val: 0.2533 time: 0.0111s\n",
      "Epoch: 0111 loss_train: 1.1210 acc_train: 0.5929 loss_val: 2.2065 acc_val: 0.2533 time: 0.0111s\n",
      "Epoch: 0112 loss_train: 1.0816 acc_train: 0.6929 loss_val: 2.1211 acc_val: 0.2600 time: 0.0111s\n",
      "Epoch: 0113 loss_train: 1.0348 acc_train: 0.7143 loss_val: 1.8657 acc_val: 0.2300 time: 0.0111s\n",
      "Epoch: 0114 loss_train: 1.1148 acc_train: 0.5929 loss_val: 1.8564 acc_val: 0.2600 time: 0.0111s\n",
      "Epoch: 0115 loss_train: 1.0799 acc_train: 0.7214 loss_val: 2.1959 acc_val: 0.2633 time: 0.0111s\n",
      "Epoch: 0116 loss_train: 1.0317 acc_train: 0.7000 loss_val: 2.4335 acc_val: 0.2533 time: 0.0111s\n",
      "Epoch: 0117 loss_train: 1.0368 acc_train: 0.7214 loss_val: 2.2987 acc_val: 0.2033 time: 0.0111s\n",
      "Epoch: 0118 loss_train: 0.9889 acc_train: 0.7357 loss_val: 2.0248 acc_val: 0.2167 time: 0.0111s\n",
      "Epoch: 0119 loss_train: 1.0168 acc_train: 0.7286 loss_val: 1.9792 acc_val: 0.2267 time: 0.0111s\n",
      "Epoch: 0120 loss_train: 1.0108 acc_train: 0.7071 loss_val: 2.1805 acc_val: 0.2400 time: 0.0111s\n",
      "Epoch: 0121 loss_train: 0.9300 acc_train: 0.7357 loss_val: 2.2104 acc_val: 0.2467 time: 0.0111s\n",
      "Epoch: 0122 loss_train: 0.9782 acc_train: 0.7214 loss_val: 2.2260 acc_val: 0.2933 time: 0.0133s\n",
      "Epoch: 0123 loss_train: 0.9502 acc_train: 0.7500 loss_val: 2.3344 acc_val: 0.2400 time: 0.0111s\n",
      "Epoch: 0124 loss_train: 0.9595 acc_train: 0.7500 loss_val: 2.4048 acc_val: 0.2567 time: 0.0111s\n",
      "Epoch: 0125 loss_train: 0.8545 acc_train: 0.7929 loss_val: 2.5190 acc_val: 0.2433 time: 0.0111s\n",
      "Epoch: 0126 loss_train: 0.8850 acc_train: 0.7643 loss_val: 2.5913 acc_val: 0.2000 time: 0.0111s\n",
      "Epoch: 0127 loss_train: 0.8945 acc_train: 0.7714 loss_val: 2.4460 acc_val: 0.2333 time: 0.0111s\n",
      "Epoch: 0128 loss_train: 0.8064 acc_train: 0.8500 loss_val: 2.1929 acc_val: 0.2467 time: 0.0111s\n",
      "Epoch: 0129 loss_train: 0.8540 acc_train: 0.8143 loss_val: 2.3442 acc_val: 0.2733 time: 0.0111s\n",
      "Epoch: 0130 loss_train: 0.8163 acc_train: 0.8000 loss_val: 2.5275 acc_val: 0.2633 time: 0.0111s\n",
      "Epoch: 0131 loss_train: 0.8202 acc_train: 0.8000 loss_val: 2.7812 acc_val: 0.2733 time: 0.0111s\n",
      "Epoch: 0132 loss_train: 0.7875 acc_train: 0.7714 loss_val: 2.8990 acc_val: 0.2433 time: 0.0111s\n",
      "Epoch: 0133 loss_train: 0.7446 acc_train: 0.8214 loss_val: 2.8411 acc_val: 0.2333 time: 0.0111s\n",
      "Epoch: 0134 loss_train: 0.7243 acc_train: 0.8429 loss_val: 2.7206 acc_val: 0.1933 time: 0.0111s\n",
      "Epoch: 0135 loss_train: 0.7610 acc_train: 0.8786 loss_val: 2.6707 acc_val: 0.2200 time: 0.0111s\n",
      "Epoch: 0136 loss_train: 0.7439 acc_train: 0.8000 loss_val: 2.6347 acc_val: 0.2000 time: 0.0111s\n",
      "Epoch: 0137 loss_train: 0.7046 acc_train: 0.8714 loss_val: 2.7138 acc_val: 0.1733 time: 0.0111s\n",
      "Epoch: 0138 loss_train: 0.6931 acc_train: 0.8571 loss_val: 2.8647 acc_val: 0.1900 time: 0.0111s\n",
      "Epoch: 0139 loss_train: 0.7030 acc_train: 0.8357 loss_val: 3.0093 acc_val: 0.1433 time: 0.0111s\n",
      "Epoch: 0140 loss_train: 0.7043 acc_train: 0.8143 loss_val: 3.0288 acc_val: 0.1400 time: 0.0111s\n",
      "Epoch: 0141 loss_train: 0.6624 acc_train: 0.8714 loss_val: 3.0123 acc_val: 0.1933 time: 0.0111s\n",
      "Epoch: 0142 loss_train: 0.6501 acc_train: 0.8786 loss_val: 3.0403 acc_val: 0.2200 time: 0.0111s\n",
      "Epoch: 0143 loss_train: 0.6647 acc_train: 0.8500 loss_val: 3.0023 acc_val: 0.2133 time: 0.0111s\n",
      "Epoch: 0144 loss_train: 0.6228 acc_train: 0.8571 loss_val: 3.1872 acc_val: 0.1600 time: 0.0111s\n",
      "Epoch: 0145 loss_train: 0.6204 acc_train: 0.8714 loss_val: 3.2169 acc_val: 0.1433 time: 0.0114s\n",
      "Epoch: 0146 loss_train: 0.6233 acc_train: 0.8571 loss_val: 3.0100 acc_val: 0.2033 time: 0.0111s\n",
      "Epoch: 0147 loss_train: 0.6060 acc_train: 0.8929 loss_val: 3.0030 acc_val: 0.2300 time: 0.0111s\n",
      "Epoch: 0148 loss_train: 0.6093 acc_train: 0.8143 loss_val: 2.9995 acc_val: 0.2300 time: 0.0110s\n",
      "Epoch: 0149 loss_train: 0.6052 acc_train: 0.8429 loss_val: 3.0764 acc_val: 0.1533 time: 0.0111s\n",
      "Epoch: 0150 loss_train: 0.5825 acc_train: 0.8786 loss_val: 3.4928 acc_val: 0.2233 time: 0.0111s\n",
      "Epoch: 0151 loss_train: 0.5590 acc_train: 0.8786 loss_val: 3.4824 acc_val: 0.2633 time: 0.0111s\n",
      "Epoch: 0152 loss_train: 0.5479 acc_train: 0.9000 loss_val: 3.6242 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0153 loss_train: 0.5720 acc_train: 0.8643 loss_val: 3.6434 acc_val: 0.1300 time: 0.0111s\n",
      "Epoch: 0154 loss_train: 0.5391 acc_train: 0.8786 loss_val: 3.5681 acc_val: 0.2467 time: 0.0111s\n",
      "Epoch: 0155 loss_train: 0.5356 acc_train: 0.8929 loss_val: 3.6197 acc_val: 0.1400 time: 0.0111s\n",
      "Epoch: 0156 loss_train: 0.5065 acc_train: 0.8857 loss_val: 3.6665 acc_val: 0.1333 time: 0.0111s\n",
      "Epoch: 0157 loss_train: 0.5262 acc_train: 0.8857 loss_val: 3.7271 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0158 loss_train: 0.5400 acc_train: 0.8714 loss_val: 3.6527 acc_val: 0.1400 time: 0.0111s\n",
      "Epoch: 0159 loss_train: 0.5696 acc_train: 0.8429 loss_val: 3.5070 acc_val: 0.2400 time: 0.0111s\n",
      "Epoch: 0160 loss_train: 0.5744 acc_train: 0.8786 loss_val: 3.9698 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0161 loss_train: 0.4718 acc_train: 0.8786 loss_val: 4.1224 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0162 loss_train: 0.4593 acc_train: 0.8857 loss_val: 4.0463 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0163 loss_train: 0.4731 acc_train: 0.8786 loss_val: 3.7680 acc_val: 0.1933 time: 0.0113s\n",
      "Epoch: 0164 loss_train: 0.4676 acc_train: 0.9143 loss_val: 3.8751 acc_val: 0.1867 time: 0.0111s\n",
      "Epoch: 0165 loss_train: 0.4500 acc_train: 0.8786 loss_val: 4.0534 acc_val: 0.2400 time: 0.0111s\n",
      "Epoch: 0166 loss_train: 0.4165 acc_train: 0.8786 loss_val: 4.0816 acc_val: 0.2633 time: 0.0111s\n",
      "Epoch: 0167 loss_train: 0.4386 acc_train: 0.9000 loss_val: 3.9557 acc_val: 0.2367 time: 0.0111s\n",
      "Epoch: 0168 loss_train: 0.4413 acc_train: 0.9071 loss_val: 4.0772 acc_val: 0.2133 time: 0.0111s\n",
      "Epoch: 0169 loss_train: 0.3854 acc_train: 0.9429 loss_val: 4.1333 acc_val: 0.1867 time: 0.0111s\n",
      "Epoch: 0170 loss_train: 0.4155 acc_train: 0.9071 loss_val: 4.3193 acc_val: 0.1467 time: 0.0111s\n",
      "Epoch: 0171 loss_train: 0.3740 acc_train: 0.9071 loss_val: 4.3234 acc_val: 0.1533 time: 0.0111s\n",
      "Epoch: 0172 loss_train: 0.4215 acc_train: 0.8786 loss_val: 4.4181 acc_val: 0.1467 time: 0.0111s\n",
      "Epoch: 0173 loss_train: 0.4047 acc_train: 0.9071 loss_val: 4.3959 acc_val: 0.1767 time: 0.0111s\n",
      "Epoch: 0174 loss_train: 0.4178 acc_train: 0.8857 loss_val: 4.6694 acc_val: 0.1533 time: 0.0111s\n",
      "Epoch: 0175 loss_train: 0.3620 acc_train: 0.9143 loss_val: 4.5885 acc_val: 0.1367 time: 0.0111s\n",
      "Epoch: 0176 loss_train: 0.3526 acc_train: 0.9357 loss_val: 4.2871 acc_val: 0.1833 time: 0.0111s\n",
      "Epoch: 0177 loss_train: 0.3636 acc_train: 0.9071 loss_val: 4.2891 acc_val: 0.2000 time: 0.0111s\n",
      "Epoch: 0178 loss_train: 0.3606 acc_train: 0.9357 loss_val: 4.4190 acc_val: 0.2367 time: 0.0111s\n",
      "Epoch: 0179 loss_train: 0.3341 acc_train: 0.9786 loss_val: 4.5527 acc_val: 0.1867 time: 0.0111s\n",
      "Epoch: 0180 loss_train: 0.3415 acc_train: 0.9357 loss_val: 4.8871 acc_val: 0.1833 time: 0.0111s\n",
      "Epoch: 0181 loss_train: 0.3372 acc_train: 0.9571 loss_val: 4.8848 acc_val: 0.1533 time: 0.0113s\n",
      "Epoch: 0182 loss_train: 0.3268 acc_train: 0.9500 loss_val: 4.7808 acc_val: 0.1767 time: 0.0111s\n",
      "Epoch: 0183 loss_train: 0.3395 acc_train: 0.9429 loss_val: 4.7672 acc_val: 0.1800 time: 0.0111s\n",
      "Epoch: 0184 loss_train: 0.3277 acc_train: 0.9429 loss_val: 4.8528 acc_val: 0.1667 time: 0.0111s\n",
      "Epoch: 0185 loss_train: 0.3227 acc_train: 0.9500 loss_val: 4.9703 acc_val: 0.1433 time: 0.0111s\n",
      "Epoch: 0186 loss_train: 0.3231 acc_train: 0.9357 loss_val: 5.1008 acc_val: 0.1500 time: 0.0111s\n",
      "Epoch: 0187 loss_train: 0.3376 acc_train: 0.9500 loss_val: 5.1237 acc_val: 0.1600 time: 0.0111s\n",
      "Epoch: 0188 loss_train: 0.3023 acc_train: 0.9500 loss_val: 5.0024 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0189 loss_train: 0.2872 acc_train: 0.9643 loss_val: 4.8206 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0190 loss_train: 0.3041 acc_train: 0.9500 loss_val: 5.0288 acc_val: 0.1067 time: 0.0111s\n",
      "Epoch: 0191 loss_train: 0.3088 acc_train: 0.9571 loss_val: 5.0961 acc_val: 0.1067 time: 0.0111s\n",
      "Epoch: 0192 loss_train: 0.2876 acc_train: 0.9429 loss_val: 5.1493 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0193 loss_train: 0.3202 acc_train: 0.9357 loss_val: 5.3626 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0194 loss_train: 0.3078 acc_train: 0.9429 loss_val: 5.4999 acc_val: 0.1067 time: 0.0111s\n",
      "Epoch: 0195 loss_train: 0.2865 acc_train: 0.9643 loss_val: 5.4466 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0196 loss_train: 0.2735 acc_train: 0.9429 loss_val: 5.4885 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0197 loss_train: 0.2721 acc_train: 0.9857 loss_val: 5.5342 acc_val: 0.1500 time: 0.0111s\n",
      "Epoch: 0198 loss_train: 0.2677 acc_train: 0.9643 loss_val: 5.5916 acc_val: 0.1333 time: 0.0111s\n",
      "Epoch: 0199 loss_train: 0.2515 acc_train: 0.9643 loss_val: 5.5225 acc_val: 0.1333 time: 0.0114s\n",
      "Epoch: 0200 loss_train: 0.2801 acc_train: 0.9500 loss_val: 5.4598 acc_val: 0.1433 time: 0.0111s\n",
      "Epoch: 0201 loss_train: 0.2275 acc_train: 0.9714 loss_val: 5.4330 acc_val: 0.1400 time: 0.0111s\n",
      "Epoch: 0202 loss_train: 0.2702 acc_train: 0.9643 loss_val: 5.5526 acc_val: 0.1700 time: 0.0111s\n",
      "Epoch: 0203 loss_train: 0.2633 acc_train: 0.9786 loss_val: 5.7310 acc_val: 0.1400 time: 0.0111s\n",
      "Epoch: 0204 loss_train: 0.2878 acc_train: 0.9714 loss_val: 5.9398 acc_val: 0.1300 time: 0.0111s\n",
      "Epoch: 0205 loss_train: 0.2640 acc_train: 0.9714 loss_val: 6.0421 acc_val: 0.1267 time: 0.0111s\n",
      "Epoch: 0206 loss_train: 0.2674 acc_train: 0.9571 loss_val: 6.0640 acc_val: 0.1233 time: 0.0144s\n",
      "Epoch: 0207 loss_train: 0.2598 acc_train: 0.9714 loss_val: 5.8733 acc_val: 0.1333 time: 0.0114s\n",
      "Epoch: 0208 loss_train: 0.2532 acc_train: 0.9714 loss_val: 5.7441 acc_val: 0.1600 time: 0.0111s\n",
      "Epoch: 0209 loss_train: 0.2857 acc_train: 0.9786 loss_val: 5.8312 acc_val: 0.1333 time: 0.0111s\n",
      "Epoch: 0210 loss_train: 0.2533 acc_train: 0.9571 loss_val: 5.9071 acc_val: 0.1267 time: 0.0111s\n",
      "Epoch: 0211 loss_train: 0.2788 acc_train: 0.9429 loss_val: 6.0370 acc_val: 0.1267 time: 0.0111s\n",
      "Epoch: 0212 loss_train: 0.2405 acc_train: 0.9714 loss_val: 6.1263 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0213 loss_train: 0.2460 acc_train: 0.9643 loss_val: 6.0630 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0214 loss_train: 0.2306 acc_train: 0.9714 loss_val: 5.9386 acc_val: 0.1467 time: 0.0111s\n",
      "Epoch: 0215 loss_train: 0.2471 acc_train: 0.9571 loss_val: 6.1986 acc_val: 0.1300 time: 0.0111s\n",
      "Epoch: 0216 loss_train: 0.2203 acc_train: 0.9714 loss_val: 6.3370 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0217 loss_train: 0.2369 acc_train: 0.9571 loss_val: 6.2306 acc_val: 0.1333 time: 0.0115s\n",
      "Epoch: 0218 loss_train: 0.2279 acc_train: 0.9500 loss_val: 6.3732 acc_val: 0.1267 time: 0.0111s\n",
      "Epoch: 0219 loss_train: 0.2055 acc_train: 0.9643 loss_val: 6.4583 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0220 loss_train: 0.2282 acc_train: 0.9714 loss_val: 6.4748 acc_val: 0.1300 time: 0.0142s\n",
      "Epoch: 0221 loss_train: 0.2452 acc_train: 0.9643 loss_val: 6.4228 acc_val: 0.1300 time: 0.0111s\n",
      "Epoch: 0222 loss_train: 0.2008 acc_train: 0.9786 loss_val: 6.5552 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0223 loss_train: 0.2060 acc_train: 0.9643 loss_val: 6.4478 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0224 loss_train: 0.2400 acc_train: 0.9786 loss_val: 6.3912 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0225 loss_train: 0.1885 acc_train: 0.9786 loss_val: 6.2580 acc_val: 0.1333 time: 0.0110s\n",
      "Epoch: 0226 loss_train: 0.2101 acc_train: 0.9643 loss_val: 6.3479 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0227 loss_train: 0.2096 acc_train: 0.9643 loss_val: 6.4768 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0228 loss_train: 0.2522 acc_train: 0.9643 loss_val: 6.6672 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0229 loss_train: 0.2037 acc_train: 0.9643 loss_val: 6.8222 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0230 loss_train: 0.2015 acc_train: 0.9714 loss_val: 6.9820 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0231 loss_train: 0.2069 acc_train: 0.9643 loss_val: 6.9130 acc_val: 0.1067 time: 0.0111s\n",
      "Epoch: 0232 loss_train: 0.2083 acc_train: 0.9714 loss_val: 6.9501 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0233 loss_train: 0.2405 acc_train: 0.9571 loss_val: 7.0128 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0234 loss_train: 0.2064 acc_train: 0.9643 loss_val: 6.9263 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0235 loss_train: 0.2127 acc_train: 0.9714 loss_val: 6.6901 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0236 loss_train: 0.2147 acc_train: 0.9571 loss_val: 6.5852 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0237 loss_train: 0.2320 acc_train: 0.9643 loss_val: 6.7247 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0238 loss_train: 0.1811 acc_train: 0.9714 loss_val: 7.0607 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0239 loss_train: 0.2222 acc_train: 0.9857 loss_val: 6.6897 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0240 loss_train: 0.2022 acc_train: 0.9643 loss_val: 6.6396 acc_val: 0.1233 time: 0.0110s\n",
      "Epoch: 0241 loss_train: 0.2046 acc_train: 0.9643 loss_val: 6.8076 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0242 loss_train: 0.2036 acc_train: 0.9786 loss_val: 7.0792 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0243 loss_train: 0.1871 acc_train: 0.9786 loss_val: 6.9273 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0244 loss_train: 0.1762 acc_train: 0.9857 loss_val: 6.7821 acc_val: 0.1200 time: 0.0110s\n",
      "Epoch: 0245 loss_train: 0.2088 acc_train: 0.9786 loss_val: 6.7760 acc_val: 0.1233 time: 0.0110s\n",
      "Epoch: 0246 loss_train: 0.1920 acc_train: 0.9714 loss_val: 6.9251 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0247 loss_train: 0.2135 acc_train: 0.9571 loss_val: 6.9931 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0248 loss_train: 0.1680 acc_train: 0.9857 loss_val: 7.0434 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0249 loss_train: 0.1896 acc_train: 0.9929 loss_val: 7.1519 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0250 loss_train: 0.1759 acc_train: 1.0000 loss_val: 7.2499 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0251 loss_train: 0.1700 acc_train: 0.9929 loss_val: 7.1410 acc_val: 0.1233 time: 0.0110s\n",
      "Epoch: 0252 loss_train: 0.1758 acc_train: 0.9786 loss_val: 6.9786 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0253 loss_train: 0.2118 acc_train: 0.9571 loss_val: 6.9488 acc_val: 0.1267 time: 0.0111s\n",
      "Epoch: 0254 loss_train: 0.1744 acc_train: 0.9714 loss_val: 7.0068 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0255 loss_train: 0.1859 acc_train: 0.9714 loss_val: 7.0613 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0256 loss_train: 0.1868 acc_train: 0.9786 loss_val: 7.2209 acc_val: 0.1233 time: 0.0110s\n",
      "Epoch: 0257 loss_train: 0.1987 acc_train: 0.9714 loss_val: 7.3135 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0258 loss_train: 0.1863 acc_train: 0.9714 loss_val: 7.3969 acc_val: 0.1233 time: 0.0110s\n",
      "Epoch: 0259 loss_train: 0.1965 acc_train: 0.9643 loss_val: 7.3762 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0260 loss_train: 0.1836 acc_train: 0.9929 loss_val: 7.2405 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0261 loss_train: 0.1714 acc_train: 0.9857 loss_val: 7.2074 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0262 loss_train: 0.1753 acc_train: 0.9714 loss_val: 7.2049 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0263 loss_train: 0.1926 acc_train: 0.9786 loss_val: 7.3018 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0264 loss_train: 0.1876 acc_train: 0.9643 loss_val: 7.4267 acc_val: 0.1200 time: 0.0110s\n",
      "Epoch: 0265 loss_train: 0.1648 acc_train: 0.9929 loss_val: 7.5429 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0266 loss_train: 0.1664 acc_train: 0.9714 loss_val: 7.4981 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0267 loss_train: 0.1878 acc_train: 0.9857 loss_val: 7.5216 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0268 loss_train: 0.1686 acc_train: 0.9786 loss_val: 7.4633 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0269 loss_train: 0.1531 acc_train: 0.9857 loss_val: 7.4838 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0270 loss_train: 0.1616 acc_train: 0.9786 loss_val: 7.5220 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0271 loss_train: 0.1859 acc_train: 0.9857 loss_val: 7.3682 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0272 loss_train: 0.1450 acc_train: 0.9929 loss_val: 7.3495 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0273 loss_train: 0.1596 acc_train: 0.9857 loss_val: 7.4705 acc_val: 0.1233 time: 0.0110s\n",
      "Epoch: 0274 loss_train: 0.1737 acc_train: 0.9714 loss_val: 7.7744 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0275 loss_train: 0.2022 acc_train: 0.9643 loss_val: 7.7552 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0276 loss_train: 0.1811 acc_train: 0.9857 loss_val: 7.7719 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0277 loss_train: 0.1547 acc_train: 0.9714 loss_val: 7.8561 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0278 loss_train: 0.1413 acc_train: 0.9929 loss_val: 7.8790 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0279 loss_train: 0.1977 acc_train: 0.9571 loss_val: 7.8248 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0280 loss_train: 0.1422 acc_train: 0.9786 loss_val: 7.8551 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0281 loss_train: 0.1488 acc_train: 0.9929 loss_val: 7.8902 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0282 loss_train: 0.1403 acc_train: 0.9929 loss_val: 7.9815 acc_val: 0.1167 time: 0.0110s\n",
      "Epoch: 0283 loss_train: 0.1317 acc_train: 0.9929 loss_val: 8.1277 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0284 loss_train: 0.1973 acc_train: 0.9643 loss_val: 8.1269 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0285 loss_train: 0.1610 acc_train: 0.9786 loss_val: 8.1897 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0286 loss_train: 0.1245 acc_train: 0.9929 loss_val: 8.1069 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0287 loss_train: 0.1370 acc_train: 0.9929 loss_val: 8.0227 acc_val: 0.1167 time: 0.0110s\n",
      "Epoch: 0288 loss_train: 0.1401 acc_train: 0.9857 loss_val: 8.0320 acc_val: 0.1200 time: 0.0110s\n",
      "Epoch: 0289 loss_train: 0.1477 acc_train: 0.9857 loss_val: 8.0766 acc_val: 0.1167 time: 0.0112s\n",
      "Epoch: 0290 loss_train: 0.1557 acc_train: 0.9714 loss_val: 8.1203 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0291 loss_train: 0.1527 acc_train: 0.9786 loss_val: 8.1624 acc_val: 0.1133 time: 0.0123s\n",
      "Epoch: 0292 loss_train: 0.1630 acc_train: 0.9857 loss_val: 8.1218 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0293 loss_train: 0.1346 acc_train: 0.9929 loss_val: 8.0706 acc_val: 0.1167 time: 0.0110s\n",
      "Epoch: 0294 loss_train: 0.1207 acc_train: 1.0000 loss_val: 8.1260 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0295 loss_train: 0.1687 acc_train: 0.9786 loss_val: 8.1999 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0296 loss_train: 0.1532 acc_train: 0.9857 loss_val: 8.1430 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0297 loss_train: 0.1273 acc_train: 0.9786 loss_val: 8.1210 acc_val: 0.1200 time: 0.0111s\n",
      "Epoch: 0298 loss_train: 0.1348 acc_train: 0.9786 loss_val: 8.1865 acc_val: 0.1167 time: 0.0110s\n",
      "Epoch: 0299 loss_train: 0.1453 acc_train: 1.0000 loss_val: 8.2682 acc_val: 0.1167 time: 0.0110s\n",
      "Epoch: 0300 loss_train: 0.1660 acc_train: 0.9786 loss_val: 8.2879 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0301 loss_train: 0.1319 acc_train: 0.9929 loss_val: 8.3020 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0302 loss_train: 0.1429 acc_train: 0.9786 loss_val: 8.3333 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0303 loss_train: 0.1526 acc_train: 0.9786 loss_val: 8.2837 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0304 loss_train: 0.1758 acc_train: 0.9929 loss_val: 8.3677 acc_val: 0.1200 time: 0.0110s\n",
      "Epoch: 0305 loss_train: 0.1264 acc_train: 0.9929 loss_val: 8.4665 acc_val: 0.1167 time: 0.0110s\n",
      "Epoch: 0306 loss_train: 0.1368 acc_train: 0.9929 loss_val: 8.5585 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0307 loss_train: 0.1645 acc_train: 0.9857 loss_val: 8.4768 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0308 loss_train: 0.1433 acc_train: 0.9929 loss_val: 8.4864 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0309 loss_train: 0.1366 acc_train: 1.0000 loss_val: 8.4803 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0310 loss_train: 0.1369 acc_train: 0.9857 loss_val: 8.7209 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0311 loss_train: 0.1275 acc_train: 0.9929 loss_val: 8.7505 acc_val: 0.1167 time: 0.0110s\n",
      "Epoch: 0312 loss_train: 0.1672 acc_train: 0.9857 loss_val: 8.8835 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0313 loss_train: 0.1497 acc_train: 0.9857 loss_val: 8.8411 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0314 loss_train: 0.1530 acc_train: 0.9857 loss_val: 8.7426 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0315 loss_train: 0.1533 acc_train: 0.9857 loss_val: 8.6528 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0316 loss_train: 0.1506 acc_train: 0.9714 loss_val: 8.8554 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0317 loss_train: 0.1213 acc_train: 0.9857 loss_val: 8.8372 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0318 loss_train: 0.1513 acc_train: 0.9786 loss_val: 8.7536 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0319 loss_train: 0.1398 acc_train: 0.9857 loss_val: 8.7896 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0320 loss_train: 0.1321 acc_train: 0.9857 loss_val: 8.8766 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0321 loss_train: 0.1326 acc_train: 0.9929 loss_val: 8.8039 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0322 loss_train: 0.1440 acc_train: 0.9786 loss_val: 8.6680 acc_val: 0.1200 time: 0.0110s\n",
      "Epoch: 0323 loss_train: 0.1229 acc_train: 0.9929 loss_val: 8.7994 acc_val: 0.1167 time: 0.0110s\n",
      "Epoch: 0324 loss_train: 0.1505 acc_train: 0.9643 loss_val: 8.8822 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0325 loss_train: 0.1523 acc_train: 0.9643 loss_val: 8.9845 acc_val: 0.1133 time: 0.0112s\n",
      "Epoch: 0326 loss_train: 0.1181 acc_train: 0.9929 loss_val: 8.9535 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0327 loss_train: 0.1337 acc_train: 0.9786 loss_val: 8.8880 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0328 loss_train: 0.1084 acc_train: 0.9929 loss_val: 8.8509 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0329 loss_train: 0.1387 acc_train: 0.9929 loss_val: 8.9237 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0330 loss_train: 0.1323 acc_train: 1.0000 loss_val: 8.9711 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0331 loss_train: 0.1258 acc_train: 0.9929 loss_val: 9.0324 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0332 loss_train: 0.1451 acc_train: 0.9857 loss_val: 9.0515 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0333 loss_train: 0.1317 acc_train: 0.9786 loss_val: 9.0925 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0334 loss_train: 0.1067 acc_train: 0.9929 loss_val: 9.1423 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0335 loss_train: 0.1173 acc_train: 0.9929 loss_val: 9.2520 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0336 loss_train: 0.1211 acc_train: 0.9857 loss_val: 9.2639 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0337 loss_train: 0.1195 acc_train: 0.9857 loss_val: 9.1788 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0338 loss_train: 0.1408 acc_train: 0.9929 loss_val: 9.1012 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0339 loss_train: 0.1200 acc_train: 0.9786 loss_val: 9.0166 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0340 loss_train: 0.1483 acc_train: 0.9929 loss_val: 9.0596 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0341 loss_train: 0.1282 acc_train: 0.9857 loss_val: 9.1741 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0342 loss_train: 0.1366 acc_train: 0.9857 loss_val: 9.2417 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0343 loss_train: 0.1339 acc_train: 0.9929 loss_val: 9.2322 acc_val: 0.1133 time: 0.0113s\n",
      "Epoch: 0344 loss_train: 0.1334 acc_train: 0.9857 loss_val: 9.2271 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0345 loss_train: 0.1128 acc_train: 0.9929 loss_val: 9.2277 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0346 loss_train: 0.1543 acc_train: 0.9857 loss_val: 9.3948 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0347 loss_train: 0.1167 acc_train: 0.9929 loss_val: 9.5180 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0348 loss_train: 0.1259 acc_train: 0.9857 loss_val: 9.5416 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0349 loss_train: 0.1479 acc_train: 0.9857 loss_val: 9.5675 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0350 loss_train: 0.1147 acc_train: 0.9929 loss_val: 9.5169 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0351 loss_train: 0.1593 acc_train: 0.9857 loss_val: 9.5267 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0352 loss_train: 0.1312 acc_train: 0.9786 loss_val: 9.7058 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0353 loss_train: 0.1188 acc_train: 0.9929 loss_val: 9.6037 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0354 loss_train: 0.1298 acc_train: 0.9857 loss_val: 9.5156 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0355 loss_train: 0.1139 acc_train: 1.0000 loss_val: 9.4858 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0356 loss_train: 0.1452 acc_train: 0.9857 loss_val: 9.6059 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0357 loss_train: 0.1036 acc_train: 0.9929 loss_val: 9.6795 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0358 loss_train: 0.1163 acc_train: 0.9929 loss_val: 9.6479 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0359 loss_train: 0.1223 acc_train: 0.9929 loss_val: 9.5837 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0360 loss_train: 0.1298 acc_train: 0.9929 loss_val: 9.5408 acc_val: 0.1100 time: 0.0111s\n",
      "Epoch: 0361 loss_train: 0.1054 acc_train: 0.9929 loss_val: 9.5880 acc_val: 0.1133 time: 0.0113s\n",
      "Epoch: 0362 loss_train: 0.1184 acc_train: 0.9857 loss_val: 9.6219 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0363 loss_train: 0.1207 acc_train: 0.9786 loss_val: 9.6610 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0364 loss_train: 0.1173 acc_train: 1.0000 loss_val: 9.7137 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0365 loss_train: 0.1157 acc_train: 1.0000 loss_val: 9.8464 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0366 loss_train: 0.1166 acc_train: 0.9857 loss_val: 9.8326 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0367 loss_train: 0.1286 acc_train: 0.9786 loss_val: 9.8364 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0368 loss_train: 0.1301 acc_train: 0.9929 loss_val: 9.8133 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0369 loss_train: 0.1259 acc_train: 0.9857 loss_val: 9.8518 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0370 loss_train: 0.1111 acc_train: 0.9929 loss_val: 9.8687 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0371 loss_train: 0.1208 acc_train: 0.9929 loss_val: 9.8333 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0372 loss_train: 0.1399 acc_train: 0.9929 loss_val: 9.8522 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0373 loss_train: 0.0931 acc_train: 1.0000 loss_val: 9.8067 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0374 loss_train: 0.1250 acc_train: 0.9714 loss_val: 9.8591 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0375 loss_train: 0.1246 acc_train: 0.9714 loss_val: 9.8571 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0376 loss_train: 0.1456 acc_train: 0.9643 loss_val: 9.9049 acc_val: 0.1133 time: 0.0113s\n",
      "Epoch: 0377 loss_train: 0.1196 acc_train: 1.0000 loss_val: 9.9518 acc_val: 0.1133 time: 0.0112s\n",
      "Epoch: 0378 loss_train: 0.1082 acc_train: 0.9857 loss_val: 9.9233 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0379 loss_train: 0.1253 acc_train: 0.9929 loss_val: 9.9011 acc_val: 0.1133 time: 0.0113s\n",
      "Epoch: 0380 loss_train: 0.1147 acc_train: 1.0000 loss_val: 9.8641 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0381 loss_train: 0.1038 acc_train: 1.0000 loss_val: 9.8552 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0382 loss_train: 0.1132 acc_train: 0.9786 loss_val: 9.8150 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0383 loss_train: 0.1259 acc_train: 0.9643 loss_val: 9.9328 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0384 loss_train: 0.1171 acc_train: 0.9929 loss_val: 9.9404 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0385 loss_train: 0.1328 acc_train: 0.9714 loss_val: 9.8813 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0386 loss_train: 0.1323 acc_train: 0.9929 loss_val: 9.8766 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0387 loss_train: 0.1200 acc_train: 1.0000 loss_val: 9.9141 acc_val: 0.1167 time: 0.0111s\n",
      "Epoch: 0388 loss_train: 0.1183 acc_train: 0.9929 loss_val: 9.9715 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0389 loss_train: 0.1162 acc_train: 0.9786 loss_val: 9.9430 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0390 loss_train: 0.0945 acc_train: 0.9929 loss_val: 9.9861 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0391 loss_train: 0.1536 acc_train: 0.9714 loss_val: 10.1031 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0392 loss_train: 0.1015 acc_train: 0.9714 loss_val: 10.0731 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0393 loss_train: 0.1129 acc_train: 0.9857 loss_val: 10.0307 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0394 loss_train: 0.1129 acc_train: 0.9929 loss_val: 9.8979 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0395 loss_train: 0.1452 acc_train: 0.9786 loss_val: 10.0715 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0396 loss_train: 0.0968 acc_train: 0.9929 loss_val: 10.2475 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0397 loss_train: 0.1154 acc_train: 0.9929 loss_val: 10.2543 acc_val: 0.1133 time: 0.0113s\n",
      "Epoch: 0398 loss_train: 0.1077 acc_train: 0.9929 loss_val: 10.1145 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0399 loss_train: 0.0928 acc_train: 0.9929 loss_val: 10.0390 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0400 loss_train: 0.1175 acc_train: 0.9929 loss_val: 10.0866 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0401 loss_train: 0.1204 acc_train: 0.9714 loss_val: 10.1317 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0402 loss_train: 0.1063 acc_train: 0.9857 loss_val: 10.0679 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0403 loss_train: 0.1100 acc_train: 0.9857 loss_val: 10.2283 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0404 loss_train: 0.1112 acc_train: 0.9929 loss_val: 10.2636 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0405 loss_train: 0.1463 acc_train: 0.9714 loss_val: 10.1950 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0406 loss_train: 0.1138 acc_train: 0.9857 loss_val: 10.2435 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0407 loss_train: 0.1085 acc_train: 0.9929 loss_val: 10.3207 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0408 loss_train: 0.1521 acc_train: 0.9714 loss_val: 10.3365 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0409 loss_train: 0.1014 acc_train: 1.0000 loss_val: 10.2729 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0410 loss_train: 0.3119 acc_train: 0.8929 loss_val: 9.8800 acc_val: 0.1100 time: 0.0111s\n",
      "Epoch: 0411 loss_train: 0.3797 acc_train: 0.9214 loss_val: 10.6817 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0412 loss_train: 0.1324 acc_train: 0.9786 loss_val: 15.7691 acc_val: 0.0667 time: 0.0111s\n",
      "Epoch: 0413 loss_train: 0.2767 acc_train: 0.9429 loss_val: 11.7513 acc_val: 0.1133 time: 0.0111s\n",
      "Epoch: 0414 loss_train: 0.5625 acc_train: 0.7714 loss_val: 12.3276 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0415 loss_train: 0.3994 acc_train: 0.8429 loss_val: 11.9863 acc_val: 0.1133 time: 0.0113s\n",
      "Epoch: 0416 loss_train: 0.3550 acc_train: 0.8857 loss_val: 10.9592 acc_val: 0.1067 time: 0.0111s\n",
      "Epoch: 0417 loss_train: 0.2781 acc_train: 0.9500 loss_val: 10.1948 acc_val: 0.0833 time: 0.0135s\n",
      "Epoch: 0418 loss_train: 0.3372 acc_train: 0.9143 loss_val: 8.4826 acc_val: 0.1233 time: 0.0111s\n",
      "Epoch: 0419 loss_train: 1.4780 acc_train: 0.6643 loss_val: 9.3463 acc_val: 0.0700 time: 0.0110s\n",
      "Epoch: 0420 loss_train: 1.0768 acc_train: 0.6357 loss_val: 8.9161 acc_val: 0.0967 time: 0.0111s\n",
      "Epoch: 0421 loss_train: 2.3905 acc_train: 0.5000 loss_val: 6.6458 acc_val: 0.1033 time: 0.0111s\n",
      "Epoch: 0422 loss_train: 2.8512 acc_train: 0.2500 loss_val: 6.5462 acc_val: 0.1933 time: 0.0110s\n",
      "Epoch: 0423 loss_train: 2.5670 acc_train: 0.3786 loss_val: 2.9334 acc_val: 0.3700 time: 0.0110s\n",
      "Epoch: 0424 loss_train: 4.5874 acc_train: 0.1214 loss_val: 4.5185 acc_val: 0.0967 time: 0.0110s\n",
      "Epoch: 0425 loss_train: 3.2343 acc_train: 0.2786 loss_val: 3.2308 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0426 loss_train: 3.6587 acc_train: 0.2786 loss_val: 3.6053 acc_val: 0.2000 time: 0.0143s\n",
      "Epoch: 0427 loss_train: 4.0013 acc_train: 0.2000 loss_val: 5.5748 acc_val: 0.1567 time: 0.0111s\n",
      "Epoch: 0428 loss_train: 3.4092 acc_train: 0.2000 loss_val: 4.5685 acc_val: 0.1267 time: 0.0112s\n",
      "Epoch: 0429 loss_train: 4.9786 acc_train: 0.1500 loss_val: 3.5061 acc_val: 0.0633 time: 0.0111s\n",
      "Epoch: 0430 loss_train: 3.4901 acc_train: 0.1214 loss_val: 2.5526 acc_val: 0.1033 time: 0.0110s\n",
      "Epoch: 0431 loss_train: 2.7849 acc_train: 0.2000 loss_val: 2.1890 acc_val: 0.1033 time: 0.0110s\n",
      "Epoch: 0432 loss_train: 2.7414 acc_train: 0.1357 loss_val: 2.3655 acc_val: 0.1833 time: 0.0110s\n",
      "Epoch: 0433 loss_train: 2.8730 acc_train: 0.1857 loss_val: 2.8186 acc_val: 0.2033 time: 0.0111s\n",
      "Epoch: 0434 loss_train: 3.6535 acc_train: 0.1286 loss_val: 3.2203 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0435 loss_train: 3.8048 acc_train: 0.1714 loss_val: 3.2178 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0436 loss_train: 4.3593 acc_train: 0.1214 loss_val: 2.9917 acc_val: 0.0833 time: 0.0110s\n",
      "Epoch: 0437 loss_train: 4.8179 acc_train: 0.1571 loss_val: 2.8380 acc_val: 0.0667 time: 0.0110s\n",
      "Epoch: 0438 loss_train: 4.4988 acc_train: 0.1214 loss_val: 2.7252 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0439 loss_train: 5.0886 acc_train: 0.1286 loss_val: 2.8877 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0440 loss_train: 4.6485 acc_train: 0.2214 loss_val: 3.1407 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0441 loss_train: 4.1471 acc_train: 0.2643 loss_val: 3.0730 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0442 loss_train: 3.6101 acc_train: 0.2857 loss_val: 2.9243 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0443 loss_train: 3.1261 acc_train: 0.2714 loss_val: 5.4226 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0444 loss_train: 2.9302 acc_train: 0.1286 loss_val: 2.0210 acc_val: 0.1033 time: 0.0110s\n",
      "Epoch: 0445 loss_train: 2.2977 acc_train: 0.1571 loss_val: 1.9040 acc_val: 0.3467 time: 0.0111s\n",
      "Epoch: 0446 loss_train: 2.2639 acc_train: 0.1571 loss_val: 2.9141 acc_val: 0.1600 time: 0.0110s\n",
      "Epoch: 0447 loss_train: 2.1901 acc_train: 0.1571 loss_val: 6.3958 acc_val: 0.0667 time: 0.0111s\n",
      "Epoch: 0448 loss_train: 2.1578 acc_train: 0.1429 loss_val: 1.9894 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0449 loss_train: 2.1837 acc_train: 0.1500 loss_val: 2.1314 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0450 loss_train: 2.2411 acc_train: 0.2214 loss_val: 2.2237 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0451 loss_train: 2.3730 acc_train: 0.1929 loss_val: 2.1867 acc_val: 0.1567 time: 0.0113s\n",
      "Epoch: 0452 loss_train: 2.3374 acc_train: 0.2071 loss_val: 2.0674 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0453 loss_train: 2.2302 acc_train: 0.2429 loss_val: 1.9501 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0454 loss_train: 2.0662 acc_train: 0.2500 loss_val: 1.8757 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0455 loss_train: 2.0514 acc_train: 0.2857 loss_val: 1.8527 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0456 loss_train: 1.9838 acc_train: 0.2929 loss_val: 2.1538 acc_val: 0.3467 time: 0.0110s\n",
      "Epoch: 0457 loss_train: 1.9553 acc_train: 0.2857 loss_val: 2.5651 acc_val: 0.1533 time: 0.0110s\n",
      "Epoch: 0458 loss_train: 1.9866 acc_train: 0.2500 loss_val: 1.9930 acc_val: 0.4167 time: 0.0110s\n",
      "Epoch: 0459 loss_train: 2.0589 acc_train: 0.2286 loss_val: 2.2609 acc_val: 0.3467 time: 0.0110s\n",
      "Epoch: 0460 loss_train: 1.9634 acc_train: 0.2714 loss_val: 1.8455 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0461 loss_train: 1.9450 acc_train: 0.2571 loss_val: 1.8445 acc_val: 0.3500 time: 0.0116s\n",
      "Epoch: 0462 loss_train: 1.8486 acc_train: 0.2929 loss_val: 1.8429 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0463 loss_train: 1.9280 acc_train: 0.2571 loss_val: 1.8356 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0464 loss_train: 1.9260 acc_train: 0.2643 loss_val: 1.8252 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0465 loss_train: 2.0214 acc_train: 0.2786 loss_val: 1.8125 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0466 loss_train: 1.9264 acc_train: 0.2714 loss_val: 1.8036 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0467 loss_train: 1.9155 acc_train: 0.2643 loss_val: 1.7981 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0468 loss_train: 1.8743 acc_train: 0.2500 loss_val: 1.7959 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0469 loss_train: 1.8597 acc_train: 0.2786 loss_val: 1.7978 acc_val: 0.3500 time: 0.0113s\n",
      "Epoch: 0470 loss_train: 1.9252 acc_train: 0.2214 loss_val: 1.8056 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0471 loss_train: 1.8045 acc_train: 0.2857 loss_val: 1.8165 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0472 loss_train: 1.8932 acc_train: 0.2786 loss_val: 1.8877 acc_val: 0.3467 time: 0.0110s\n",
      "Epoch: 0473 loss_train: 1.8690 acc_train: 0.2429 loss_val: 1.9711 acc_val: 0.3467 time: 0.0110s\n",
      "Epoch: 0474 loss_train: 1.8437 acc_train: 0.2857 loss_val: 2.0497 acc_val: 0.3533 time: 0.0110s\n",
      "Epoch: 0475 loss_train: 1.8544 acc_train: 0.2429 loss_val: 2.1371 acc_val: 0.2767 time: 0.0110s\n",
      "Epoch: 0476 loss_train: 1.8319 acc_train: 0.2643 loss_val: 2.2227 acc_val: 0.1133 time: 0.0110s\n",
      "Epoch: 0477 loss_train: 1.8097 acc_train: 0.2857 loss_val: 2.2738 acc_val: 0.0833 time: 0.0111s\n",
      "Epoch: 0478 loss_train: 1.8496 acc_train: 0.2786 loss_val: 2.3548 acc_val: 0.1233 time: 0.0110s\n",
      "Epoch: 0479 loss_train: 1.8111 acc_train: 0.2929 loss_val: 2.4513 acc_val: 0.1500 time: 0.0110s\n",
      "Epoch: 0480 loss_train: 1.8034 acc_train: 0.2929 loss_val: 2.5590 acc_val: 0.0600 time: 0.0110s\n",
      "Epoch: 0481 loss_train: 1.9197 acc_train: 0.2500 loss_val: 2.6129 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0482 loss_train: 1.8219 acc_train: 0.2929 loss_val: 2.7356 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0483 loss_train: 1.8096 acc_train: 0.2929 loss_val: 2.9066 acc_val: 0.1567 time: 0.0111s\n",
      "Epoch: 0484 loss_train: 1.8165 acc_train: 0.2929 loss_val: 3.0599 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0485 loss_train: 1.8041 acc_train: 0.2929 loss_val: 3.0243 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0486 loss_train: 1.8195 acc_train: 0.2929 loss_val: 3.0416 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0487 loss_train: 1.7989 acc_train: 0.2929 loss_val: 2.9920 acc_val: 0.1567 time: 0.0113s\n",
      "Epoch: 0488 loss_train: 1.8028 acc_train: 0.2929 loss_val: 2.7908 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0489 loss_train: 1.8015 acc_train: 0.2929 loss_val: 2.5559 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0490 loss_train: 1.8188 acc_train: 0.2929 loss_val: 2.4799 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0491 loss_train: 1.8223 acc_train: 0.2929 loss_val: 2.4018 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0492 loss_train: 1.8117 acc_train: 0.2929 loss_val: 2.3480 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0493 loss_train: 1.8623 acc_train: 0.2357 loss_val: 2.3577 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0494 loss_train: 1.8358 acc_train: 0.3000 loss_val: 2.4060 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0495 loss_train: 1.8098 acc_train: 0.2857 loss_val: 2.4480 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0496 loss_train: 1.7985 acc_train: 0.2857 loss_val: 2.4568 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0497 loss_train: 1.8109 acc_train: 0.2500 loss_val: 2.4575 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0498 loss_train: 1.8102 acc_train: 0.2786 loss_val: 2.4561 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0499 loss_train: 1.8190 acc_train: 0.2643 loss_val: 2.4512 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0500 loss_train: 1.8258 acc_train: 0.2929 loss_val: 2.4562 acc_val: 0.1567 time: 0.0110s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 5.9862s\n",
      "Test set results: loss= 2.5312 accuracy= 0.1370\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=500,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=1,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "# adj, features,\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCNWithKAN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda18fd-811b-46a8-974a-5931e0cc4d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990b97d-642b-4ad1-8537-78bf763d370e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
